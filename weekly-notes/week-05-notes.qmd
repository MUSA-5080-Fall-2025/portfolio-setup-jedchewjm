---
title: "Week 5 Notes - Intro to Linear Regression"
date: "2025-10-06"
---

## Key Concepts Learned

1.  **The Statistical Learning Framework:** What are we actually doing?
2.  **Two goals:** Understanding relationships vs Making predictions
3.  **Building your first model** with PA census data
4.  **Model evaluation:** How do we know if it's any good?
5.  **Checking assumptions:** When can we trust the model?
6.  **Improving predictions:** Transformations, multiple variables

### **Statistical Significance**

**t-statistic:** How many standard errors away from 0?

-   Bigger \|t\| = more confidence the relationship is real

**p-value:** Probability of seeing our estimate if H₀ is true

-   Small p → reject H₀, conclude relationship exists

### Model Evaluation

-   **How well does it fit the data we used?** (in-sample fit using R²)

-   **How well would it predict new data?** (out-of-sample performance)

### Checking Assumptions: Residuals

Residuals = observed − fitted. They’re your best proxy for the error term. Good models leave residuals that look like random noise.

## Coding Techniques

-   **Linear Regression Train/Test Split**

```{r, eval = FALSE}
set.seed(123)
n <- nrow(pa_data)

# 70% training, 30% testing
train_indices <- sample(1:n, size = 0.7 * n)
train_data <- pa_data[train_indices, ]
test_data <- pa_data[-train_indices, ]

# Fit on training data only
model_train <- lm(median_incomeE ~ total_popE, data = train_data)

# Predict on test data
test_predictions <- predict(model_train, newdata = test_data)
```

-   **Evaluate Predictions**

```{r, eval = FALSE}
rmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))
rmse_train <- summary(model_train)$sigma

cat("Training RMSE:", round(rmse_train, 0), "\n")
cat("Test RMSE:", round(rmse_test, 0), "\n")
```

-   **10-Fold Cross Validation**

```{r, eval= FALSE}
library(caret)

train_control <- trainControl(method = "cv", number = 10)
cv_model <- train(median_incomeE ~ total_popE,
                  data = pa_data,
                  method = "lm",
                  trControl = train_control)
cv_model$results
```

-   **Residual Plot**

```{r, eval = FALSE}
pa_data$residuals <- residuals(model1)
pa_data$fitted <- fitted(model1)

ggplot(pa_data, aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residual Plot", x = "Fitted Values", y = "Residuals") +
  theme_minimal()
```

**Residuals vs Fitted**

![](images/clipboard-3467357846.png)

-   **Goal:** a random, horizontal band around 0. Residual plots should show **random scatter** - any pattern means your model is missing something systematic

-   **Red flags:**

    -   Curvature → missing nonlinear term (try polynomials/splines or transform xxx).

    -   Funnel/wedge shape → heteroskedasticity (use log/Box–Cox on yyy, weighted least squares, or robust SEs).

    -   Clusters/bands → omitted categorical/group effects or interaction terms.

**Constant Variance and Heteroskedacity**

-   Heteroskedasticity is a condition in regression analysis where the variance of the error terms is not constant but changes as the value of one or more independent variables changes

    -   Model fits well for some values (e.g., small counties) but poorly for others (large counties)

    -   May indicate **missing variables** that matter more at certain X values

    -   Ask: “What’s different about observations with large residuals?”

**Formal Test: Breusch-Pagan**

```{r, eval = FALSE}
library(lmtest) 
bptest(model1)
```

**Interpretation:**

-   **p \> 0.05:** Constant variance assumption OK
-   **p \< 0.05:** Evidence of heteroscedasticity

**If detected, solutions:**

1.  Transform Y (try `log(income)`)
2.  Robust standard errors
3.  Add missing variables
4.  Accept it (point predictions still OK for prediction goals)

## Questions & Challenges

-   

## Connections to Policy

-   

## Reflection

-   
