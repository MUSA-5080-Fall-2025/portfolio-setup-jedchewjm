{
  "hash": "c9a323251d64822058ec29775aa79768",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 5 Notes - Intro to Linear Regression\"\ndate: \"2025-10-06\"\n---\n\n## Key Concepts Learned\n\n1.  **The Statistical Learning Framework:** What are we actually doing?\n2.  **Two goals:** Understanding relationships vs Making predictions\n3.  **Building your first model** with PA census data\n4.  **Model evaluation:** How do we know if it's any good?\n5.  **Checking assumptions:** When can we trust the model?\n6.  **Improving predictions:** Transformations, multiple variables\n\n### **Statistical Significance**\n\n**t-statistic:** How many standard errors away from 0?\n\n-   Bigger \\|t\\| = more confidence the relationship is real\n\n**p-value:** Probability of seeing our estimate if H₀ is true\n\n-   Small p → reject H₀, conclude relationship exists\n\n### Model Evaluation\n\n-   **How well does it fit the data we used?** (in-sample fit using R²)\n\n-   **How well would it predict new data?** (out-of-sample performance)\n\n### Checking Assumptions: Plots reveal what $$R^2$$ hides\n\n-   **Assumption 1: Linear Relationship**\n\n    -   Check with Residual Plot\n\n    -   Residuals = observed − fitted. They’re your best proxy for the error term. Good models leave residuals that look like random noise.\n\n-   **Assumption 2: Constant Variance**\n\n    -   Check heteroscedasticity (i.e. variance changes across X\n\n    -   Impact: standard errors are wrong -\\> p values are misleading\n\n-   **Assumption 3: Normality of Residuals**\n\n    -   Check with Q-Q Plot (quantile-quantile plot) of residuals\n\n    -   Important for confidence & prediction intervals\n\n    -   Needed for valid hypothesis tests (t-test and F-test)\n\n-   **Assumption 4: No Multicollinearity**\n\n    -   Coefficients become unstable and hard to interpret\n\n-   **Assumption 5: No Influential Outliers**\n\n    -   Influential Outliers: those with high leverage and large residuals\n\n    -   Visual Diagnostic using Cook's Distance (*Cook's D*)\n\n### Improving the Model\n\n-   Adding more predictors\n\n-   Log Transformations\n\n-   Categorical Variables\n\n## Coding Techniques\n\n-   **Linear Regression Train/Test Split**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- nrow(pa_data)\n\n# 70% training, 30% testing\ntrain_indices <- sample(1:n, size = 0.7 * n)\ntrain_data <- pa_data[train_indices, ]\ntest_data <- pa_data[-train_indices, ]\n\n# Fit on training data only\nmodel_train <- lm(median_incomeE ~ total_popE, data = train_data)\n\n# Predict on test data\ntest_predictions <- predict(model_train, newdata = test_data)\n```\n:::\n\n\n-   **Evaluate Predictions**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train <- summary(model_train)$sigma\n\ncat(\"Training RMSE:\", round(rmse_train, 0), \"\\n\")\ncat(\"Test RMSE:\", round(rmse_test, 0), \"\\n\")\n```\n:::\n\n\n-   **10-Fold Cross Validation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n\ntrain_control <- trainControl(method = \"cv\", number = 10)\ncv_model <- train(median_incomeE ~ total_popE,\n                  data = pa_data,\n                  method = \"lm\",\n                  trControl = train_control)\ncv_model$results\n```\n:::\n\n\n-   **Residual Plot**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npa_data$residuals <- residuals(model1)\npa_data$fitted <- fitted(model1)\n\nggplot(pa_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n```\n:::\n\n\n**Residuals vs Fitted**\n\n![](images/clipboard-3467357846.png){width=\"902\"}\n\n-   **Goal:** a random, horizontal band around 0. Residual plots should show **random scatter** - any pattern means your model is missing something systematic\n\n-   **Red flags:**\n\n    -   Curvature → missing nonlinear term (try polynomials/splines or transform xxx).\n\n    -   Funnel/wedge shape → heteroskedasticity (use log/Box–Cox on yyy, weighted least squares, or robust SEs).\n\n    -   Clusters/bands → omitted categorical/group effects or interaction terms.\n\n**Constant Variance and Heteroskedacity**\n\n-   Heteroskedasticity is a condition in regression analysis where the variance of the error terms is not constant but changes as the value of one or more independent variables changes\n\n    -   Model fits well for some values (e.g., small counties) but poorly for others (large counties)\n\n    -   May indicate **missing variables** that matter more at certain X values\n\n    -   Ask: “What’s different about observations with large residuals?”\n\n![](images/clipboard-3798739105.png){width=\"619\"}\n\n**Formal Test: Breusch-Pagan**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtest) \nbptest(model1)\n```\n:::\n\n\n-   **Interpretation:**\n\n    -   **p \\> 0.05:** Constant variance assumption OK\n\n    -   **p \\< 0.05:** Evidence of heteroscedasticity\n\n**If detected, solutions:**\n\n1.  Transform Y (try `log(income)`)\n2.  Robust standard errors\n3.  Add missing variables\n4.  Accept it (point predictions still OK for prediction goals)\n\n**Multicollinearity & Variance Inflation Factor (vif)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nvif(model1)  # Variance Inflation Factor\n\n# Rule of thumb: VIF > 10 suggests problems\n# Not relevant with only 1 predictor!\n```\n:::\n\n\n**Influential Outliers – Cook's D \\> 4/n**\n\n![](images/clipboard-3480630247.png){width=\"563\"}\n\n## Questions & Challenges\n\n-   I am also interested in learning more about non-parametric approaches in future weeks\n\n## Connections to Policy\n\n-   It is important to understand the assumptions behind a model. For example, heteroskedasticity is often an indicator of model misspecification – it indicates missing variables that matter more at certain X values. Population alone predicts income well in rural counties, but large urban counties need additional variables (education, industry) to predict accurately.\n\n## Reflection\n\n-   Excited to put these skills to the test with the midterm home price prediction challenge!\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}