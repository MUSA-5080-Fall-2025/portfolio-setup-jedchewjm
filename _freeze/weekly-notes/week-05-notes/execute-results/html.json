{
  "hash": "f44148437c67677e777442247e335438",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 5 Notes - Intro to Linear Regression\"\ndate: \"2025-10-06\"\n---\n\n## Key Concepts Learned\n\n1.  **The Statistical Learning Framework:** What are we actually doing?\n2.  **Two goals:** Understanding relationships vs Making predictions\n3.  **Building your first model** with PA census data\n4.  **Model evaluation:** How do we know if it's any good?\n5.  **Checking assumptions:** When can we trust the model?\n6.  **Improving predictions:** Transformations, multiple variables\n\n### **Statistical Significance**\n\n**t-statistic:** How many standard errors away from 0?\n\n-   Bigger \\|t\\| = more confidence the relationship is real\n\n**p-value:** Probability of seeing our estimate if H₀ is true\n\n-   Small p → reject H₀, conclude relationship exists\n\n### Model Evaluation\n\n-   **How well does it fit the data we used?** (in-sample fit using R²)\n\n-   **How well would it predict new data?** (out-of-sample performance)\n\n### Checking Assumptions: Residuals\n\nResiduals = observed − fitted. They’re your best proxy for the error term. Good models leave residuals that look like random noise.\n\n## Coding Techniques\n\n-   **Linear Regression Train/Test Split**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- nrow(pa_data)\n\n# 70% training, 30% testing\ntrain_indices <- sample(1:n, size = 0.7 * n)\ntrain_data <- pa_data[train_indices, ]\ntest_data <- pa_data[-train_indices, ]\n\n# Fit on training data only\nmodel_train <- lm(median_incomeE ~ total_popE, data = train_data)\n\n# Predict on test data\ntest_predictions <- predict(model_train, newdata = test_data)\n```\n:::\n\n\n-   **Evaluate Predictions**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrmse_test <- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train <- summary(model_train)$sigma\n\ncat(\"Training RMSE:\", round(rmse_train, 0), \"\\n\")\ncat(\"Test RMSE:\", round(rmse_test, 0), \"\\n\")\n```\n:::\n\n\n-   **10-Fold Cross Validation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n\ntrain_control <- trainControl(method = \"cv\", number = 10)\ncv_model <- train(median_incomeE ~ total_popE,\n                  data = pa_data,\n                  method = \"lm\",\n                  trControl = train_control)\ncv_model$results\n```\n:::\n\n\n-   **Residual Plot**\n\n\n::: {.cell}\n\n```{.r .cell-code}\npa_data$residuals <- residuals(model1)\npa_data$fitted <- fitted(model1)\n\nggplot(pa_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n```\n:::\n\n\n**Residuals vs Fitted**\n\n![](images/clipboard-3467357846.png)\n\n-   **Goal:** a random, horizontal band around 0. Residual plots should show **random scatter** - any pattern means your model is missing something systematic\n\n-   **Red flags:**\n\n    -   Curvature → missing nonlinear term (try polynomials/splines or transform xxx).\n\n    -   Funnel/wedge shape → heteroskedasticity (use log/Box–Cox on yyy, weighted least squares, or robust SEs).\n\n    -   Clusters/bands → omitted categorical/group effects or interaction terms.\n\n**Constant Variance and Heteroskedacity**\n\n-   Heteroskedasticity is a condition in regression analysis where the variance of the error terms is not constant but changes as the value of one or more independent variables changes\n\n    -   Model fits well for some values (e.g., small counties) but poorly for others (large counties)\n\n    -   May indicate **missing variables** that matter more at certain X values\n\n    -   Ask: “What’s different about observations with large residuals?”\n\n**Formal Test: Breusch-Pagan**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmtest) \nbptest(model1)\n```\n:::\n\n\n**Interpretation:**\n\n-   **p \\> 0.05:** Constant variance assumption OK\n-   **p \\< 0.05:** Evidence of heteroscedasticity\n\n**If detected, solutions:**\n\n1.  Transform Y (try `log(income)`)\n2.  Robust standard errors\n3.  Add missing variables\n4.  Accept it (point predictions still OK for prediction goals)\n\n## Questions & Challenges\n\n-   \n\n## Connections to Policy\n\n-   \n\n## Reflection\n\n-   \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}