{
  "hash": "c9a47bac7333f922c9eb19c94a1e9cd7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 10 Notes - Logistic Regression for Binary Outcomes\"\ndate: \"2025-11-10\"\n---\n\n## Key Concepts Learned\n\n### Logistic Regression – Binary Classification Problems in Policy\n\n-   **Criminal Justice:** Will someone reoffend? (recidivism) Will someone appear for court? (flight risk)\n-   **Health:** Will patient develop disease? (risk assessment) Will treatment be successful? (outcome prediction)\n-   **Economics:** Will loan default? (credit risk) Will person get hired? (employment prediction)\n-   **Urban Planning:** Will building be demolished? (blight prediction) Will household participate in program? (uptake prediction)\n\n### Fundamental Challenge: Threshold\n\n-   Cost of false positives (e.g. marking legitimate email as spam)\n-   Cost of false negatives (e.g. missing actual spam)\n\n**Confusion Matrix**\n\n![](images/clipboard-2939296991.png){width=\"238\"}\n\n-   **Sensitivity (Recall, True Positive Rate):** \\[\\text{Sensitivity} = \\frac{TP}{TP + FN}\\] *“Of all actual positives, how many did we catch?”*\n-   **Specificity (True Negative Rate):** \\[\\text{Specificity} = \\frac{TN}{TN + FP}\\] *“Of all actual negatives, how many did we correctly identify?”*\n-   **Precision (Positive Predictive Value):** \\[\\text{Precision} = \\frac{TP}{TP + FP}\\] *“Of all our positive predictions, how many were correct?”*\n\n### Disparate Impact & Algorithmic Bias in Action\n\nA model can be “accurate” overall but perform very differently across groups. Group B experiences:\n\n-   Lower sensitivity (more people who will reoffend are missed)\n-   Lower specificity (more people who won’t reoffend are flagged)\n-   Higher false positive rate (more unjust interventions)\n\n| Group   | Sensitivity | Specificity | False Positive Rate |\n|:--------|:------------|:------------|:--------------------|\n| Overall | 0.72        | 0.68        | 0.32                |\n| Group A | 0.78        | 0.74        | 0.26                |\n| Group B | 0.64        | 0.58        | 0.42                |\n\n### Framework for Threshold Selection\n\n**Step 1: Understand the consequences**\n\nWhat happens with a false positive? What happens with a false negative? Are costs symmetric or asymmetric?\n\n**Step 2: Consider stakeholder perspectives**\n\nWho is affected by each type of error? Do all groups experience consequences equally?\n\n**Step 3: Choose your metric priority**\n\nMaximize sensitivity? (catch all positives) Maximize specificity? (minimize false alarms) Balance precision and recall? (F1 score) Equalize across groups?\n\n**Step 4: Test multiple thresholds**\n\nEvaluate performance across thresholds Look at group-wise performance Consider sensitivity analysis\n\n------------------------------------------------------------------------\n\n## Coding Techniques\n\n-   **Example: Email Spam Detection**\n    -   number of exclamation marks\n\n    -   contains the word \"free\"\n\n    -   email length\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create example spam detection data\nset.seed(123)\nn_emails <- 1000\n\nspam_data <- data.frame(\n  exclamation_marks = c(rpois(100, 5), rpois(900, 0.5)),  # Spam has more !\n  contains_free = c(rbinom(100, 1, 0.8), rbinom(900, 1, 0.1)),  # Spam mentions \"free\"\n  length = c(rnorm(100, 200, 50), rnorm(900, 500, 100)),  # Spam is shorter\n  is_spam = c(rep(1, 100), rep(0, 900))\n)\n\n# Fit logistic regression\nspam_model <- glm(\n  is_spam ~ exclamation_marks + contains_free + length,\n  data = spam_data,\n  family = \"binomial\"  # This specifies logistic regression\n)\n\n# View results\nsummary(spam_model)\ncoefs <- coef(spam_model)\nodds_ratios <- exp(coefs)\nprint(odds_ratios)\n```\n:::\n\n\n-   **Confusion Matrix**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create example predictions\nset.seed(123)\nspam_data <- data.frame(\n  actual_spam = c(rep(1, 100), rep(0, 900)),\n  predicted_prob = c(rnorm(100, 0.7, 0.2), rnorm(900, 0.3, 0.2))\n) %>%\n  mutate(predicted_prob = pmax(0.01, pmin(0.99, predicted_prob)))\n\n# With threshold = 0.5\nspam_data <- spam_data %>%\n  mutate(predicted_spam = ifelse(predicted_prob > 0.5, 1, 0))\n\n# Calculate confusion matrix\nconf_mat <- confusionMatrix(\n  as.factor(spam_data$predicted_spam),\n  as.factor(spam_data$actual_spam),\n  positive = \"1\"\n)\n```\n:::\n\n\n-   **Threshold Choice**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate metrics at different thresholds\nthresholds <- seq(0.1, 0.9, by = 0.1)\n\nmetrics_by_threshold <- map_df(thresholds, function(thresh) {\n  preds <- ifelse(spam_data$predicted_prob > thresh, 1, 0)\n  cm <- confusionMatrix(as.factor(preds), as.factor(spam_data$actual_spam), \n                        positive = \"1\")\n  \n  data.frame(\n    threshold = thresh,\n    sensitivity = cm$byClass[\"Sensitivity\"],\n    specificity = cm$byClass[\"Specificity\"],\n    precision = cm$byClass[\"Precision\"]\n  )\n})\n\n# Visualize the trade-off\nggplot(metrics_by_threshold, aes(x = threshold)) +\n  geom_line(aes(y = sensitivity, color = \"Sensitivity\"), size = 1.2) +\n  geom_line(aes(y = specificity, color = \"Specificity\"), size = 1.2) +\n  geom_line(aes(y = precision, color = \"Precision\"), size = 1.2) +\n  labs(title = \"The Threshold Trade-off\",\n       subtitle = \"As threshold increases, we become more selective\",\n       x = \"Probability Threshold\", y = \"Metric Value\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n-   **ROC Curve**\n    -   Goal: illustrate trade-off between true positive rate and false positive rate\n\n    -   X-axis: False Positive Rate (1 - Specificity)\n\n    -   Y-axis: True Positive Rate (Sensitivity)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create ROC curve for our spam example\nroc_obj <- roc(spam_data$actual_spam, spam_data$predicted_prob)\n\n# Plot it\nggroc(roc_obj, color = \"steelblue\", size = 1.2) +\n  geom_abline(slope = 1, intercept = 1, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"ROC Curve: Spam Detection Model\",\n       subtitle = paste0(\"AUC = \", round(auc(roc_obj), 3)),\n       x = \"1 - Specificity (False Positive Rate)\",\n       y = \"Sensitivity (True Positive Rate)\") +\n  theme_minimal() +\n  coord_fixed()\n\n# Print AUC\nauc_value <- auc(roc_obj)\ncat(\"\\nArea Under the Curve (AUC):\", round(auc_value, 3))\n```\n:::\n\n\n**Interpreting AUC**\n\n-   **AUC = 1.0:** Perfect classifier\n-   **AUC = 0.9-1.0:** Excellent\n-   **AUC = 0.8-0.9:** Good\n-   **AUC = 0.7-0.8:** Acceptable\n\n## Questions & Challenges\n\n-   Private-sector software providers may be very hesitant to publicly share the inner workings and metrics of their predictive algorithms\n\n## Connections to Policy – Practical Recommendations\n\n1.  **Report multiple metrics** - not just accuracy\n2.  **Show the ROC curve** - demonstrates trade-offs\n3.  **Test multiple thresholds** - document your choice\n4.  **Evaluate by sub-group** - check for disparate impact\n5.  **Document assumptions** - explain why you chose your threshold\n6.  **Consider context** - what are the real-world consequences?\n7.  **Provide uncertainty** - confidence intervals, not just point estimates\n8.  **Enable recourse** - can predictions be challenged?\n\n## Reflection\n\n-   Further Reading: <https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}