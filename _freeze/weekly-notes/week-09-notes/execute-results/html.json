{
  "hash": "9e109724d05fe3f5215adfd7725452aa",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 9 Notes - Predictive Policing\"\ndate: \"2025-11-03\"\n---\n\n## Key Concepts Learned\n\n### Technical Questions\n\n-   How do we model crime counts?\n\n-   What spatial features predict crime?\n\n-   How do we validate predictions?\n\n-   Can we outperform baseline methods?\n\n![](images/clipboard-3233584324.png)\n\n### Critical Questions\n\n-   Whose data? Whose crimes?\n\n-   What if the data is “dirty”?\n\n-   Who benefits? Who is harmed?\n\n-   What feedback loops are created?\n\n-   Can technical solutions fix social problems?\n\n### Defining \"Dirty Data\"\n\nRichardson et al. 2019: “*Data derived from or influenced by corrupt, biased, and unlawful practices, including data that has been intentionally manipulated or ‘juked,’ as well as data that is distorted by individual and societal biases*.”\n\n1.  Fabricated/Manipulated Data\n2.  Systematically Biased Data\n3.  Missing/Incomplete Data\n4.  Proxy Problems\n\n### Confirmation Bias Feedback Loop:\n\n-   Algorithm learns: “Crime happens in neighborhood X”\n\n-   Police sent to neighborhood X\n\n-   More arrests in neighborhood X (regardless of actual crime)\n\n-   Algorithm “confirmed”: “We were right about neighborhood X!”\n\n-   Cycle intensifies\n\n### Modeling Workflow\n\n**01 \\| Setup & Data Preparation**\n\n-   Load burglaries (point data)\n\n-   Load abandoned cars (311 calls)\n\n-   **Create fishnet** (500m × 500m grid)\n\n-   Aggregate burglaries to cells\n\n**02 \\| Baseline Comparison**\n\n-   **Kernel Density Estimation (KDE)**\n\n-   Simple spatial smoothing\n\n-   What we need to beat!\n\n**03 \\| Feature Engineering**\n\n**Using Abandoned Cars as “Disorder Indicator”:**\n\n-   **Count** in each cell\n\n-   **k-Nearest Neighbors** (mean distance to 3 nearest)\n\n-   **LISA** (Local Moran’s I - identify hot spots)\n\n-   **Distance to hot spots** (significant clusters)\n\n**04 \\| Count Regression Models**\n\n-   Fit **Poisson regression**\n\n-   Test for **overdispersion**\n\n-   Fit **Negative Binomial** (if needed)\n\n-   Interpret coefficients\n\n**05 \\| Spatial Cross-Validation**\n\n-   **Leave-One-Group-Out (LOGO)**\n\n-   Train on n-1 districts\n\n-   Test on held-out district\n\n-   Calculate MAE/RMSE\n\n**06 \\| Model Comparison**\n\n-   Compare to KDE baseline\n\n-   Map predictions vs. actual\n\n-   Analyze errors spatially\n\n**Summary of Different Spatial Measures**\n\n-   Count → How much disorder is HERE?\n\n-   k-NN Distance → How CLOSE are we to disorder?\n\n-   Hot Spots (LISA) → Where does disorder CLUSTER?\n\n-   Distance to Hot Spots → How close to concentrated disorder?\n\n-   Each captures a different aspect of spatial proximity to our indicator variable\n\n**Common Approaches to Spatial Weights Matrix (W)**\n\n-   **Contiguity:** Share a border? (Queen vs. Rook)\n\n    -   Our fishnet grid uses Queen contiguity (most common for regular grids)\n\n-   **Distance:** Within threshold distance?\n\n-   **K-nearest neighbors:** Closest k locations\n\n**Four Types of Significant Clusters**\n\n![](images/clipboard-40446740.png)\n\n## Coding Techniques\n\n-   **Local Moran's I**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spdep)\n\n# Step 1: Create spatial object\nfishnet_sp <- as_Spatial(fishnet)\n\n# Step 2: Define neighbors (Queen contiguity)\nneighbors <- poly2nb(fishnet_sp, queen = TRUE)\n\n# Step 3: Create spatial weights (row-standardized)\nweights <- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n\n# Step 4: Calculate Local Moran's I\nlocal_moran <- localmoran(\n  fishnet$abandoned_cars,  # Variable of interest\n  weights,                  # Spatial weights\n  zero.policy = TRUE       # Handle cells with no neighbors\n)\n\n# Step 5: Extract components\nfishnet$local_I <- local_moran[, \"Ii\"]      # Local I statistic\nfishnet$p_value <- local_moran[, \"Pr(z != E(Ii))\"]  # P-value\nfishnet$z_score <- local_moran[, \"Z.Ii\"]    # Z-score\n```\n:::\n\n\n-   **Distance to Nearest Feature (kNN where k = 1)**\n\n    For each grid cell:\n\n    1.  Find location of all abandoned cars\n\n    2.  Calculate distance to each\n\n    3.  Keep minimum distance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(FNN)\n\n# Calculate distance to nearest abandoned car\nnn_dist <- get.knnx(\n  data = st_coordinates(abandoned_cars),      # \"To\" locations\n  query = st_coordinates(st_centroid(fishnet)), # \"From\" locations\n  k = 1                                          # Nearest 1\n)\n\n# Extract distances\nfishnet$abandoned_car_nn <- nn_dist$nn.dist[, 1]\n```\n:::\n\n\n-   **Distance to Hot Spot**\n\n    -   Step 1: Identify hotspots (Local Moran’s I High-High clusters)\n\n    -   Step 2: distance from each cell to nearest hotspot\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Step 1: Identify hotspots (we did this earlier)\nhotspot_cells <- filter(fishnet, hotspot == 1)\n\n# Step 2: Calculate distances\nhotspot_dist <- get.knnx(\n  data = st_coordinates(st_centroid(hotspot_cells)),\n  query = st_coordinates(st_centroid(fishnet)),\n  k = 1\n)\n\nfishnet$hotspot_nn <- hotspot_dist$nn.dist[, 1]\n```\n:::\n\n\n-   **Visualizing Distance Features**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create comparison maps\np1 <- ggplot(fishnet) +\n  geom_sf(aes(fill = abandoned_car_nn), color = NA) +\n  scale_fill_viridis_c(name = \"Distance (m)\", option = \"plasma\") +\n  labs(title = \"Distance to Nearest Abandoned Car\") +\n  theme_void()\n\np2 <- ggplot(fishnet) +\n  geom_sf(aes(fill = hotspot_nn), color = NA) +\n  scale_fill_viridis_c(name = \"Distance (m)\", option = \"magma\") +\n  labs(title = \"Distance to Nearest Hotspot\") +\n  theme_void()\n\ngrid.arrange(p1, p2, ncol = 2)\n```\n:::\n\n\n-   **Creating a Fishnet Grid**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sf)\n\n# Step 1: Define cell size (in map units - meters for our projection)\ncell_size <- 500  # 500m x 500m cells\n\n# Step 2: Create grid over study area\nfishnet <- st_make_grid(\n  chicago_boundary,\n  cellsize = cell_size,\n  square = TRUE,\n  what = \"polygons\"\n) %>%\n  st_sf() %>%\n  mutate(uniqueID = row_number())\n\n# Step 3: Clip to study area (remove cells outside boundary)\nfishnet <- fishnet[chicago_boundary, ]\n\n# Check results\nnrow(fishnet)  # Number of cells\nst_area(fishnet[1, ])  # Area of one cell (should be 250,000 m²)\n```\n:::\n\n\n-   **Aggregating Points to Grid**\n\n    -   Spatial join between crimes (points) and fishnet (polygons)\n\n    -   Count crimes per cell\n\n    -   Handle cells with zero crimes\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count burglaries per cell\nburglary_counts <- st_join(burglaries, fishnet) %>%\n  st_drop_geometry() %>%\n  group_by(uniqueID) %>%\n  summarize(countBurglaries = n())\n\n# Join back to fishnet\nfishnet <- fishnet %>%\n  left_join(burglary_counts, by = \"uniqueID\") %>%\n  mutate(countBurglaries = replace_na(countBurglaries, 0))\n\n# Summary\nsummary(fishnet$countBurglaries)\n#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#      0       0       1    2.3       3      47\n```\n:::\n\n\n**Leave-One-Group-Out (LOGO-CV) Implementation**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get unique districts\ndistricts <- unique(fishnet$District)\n\n# Initialize results\ncv_results <- list()\n\n# Loop through districts\nfor (dist in districts) {\n  train_data <- fishnet %>% filter(District != dist)  # Split data\n  test_data <- fishnet %>% filter(District == dist)\n  \n  # Fit model on training data\n  model_cv <- glm.nb(\n    countBurglaries ~ Abandoned_Cars + Abandoned_Cars.nn + abandoned.isSig.dist,\n    data = train_data\n  )\n  \n  # Predict on test data\n  test_data$prediction <- predict(model_cv, test_data, type = \"response\")\n  \n  # Store results\n  cv_results[[dist]] <- test_data\n}\n\n# Combine all predictions\nall_predictions <- bind_rows(cv_results)\n```\n:::\n\n\n**Common Error Metrics: MAE, RMSE, Bias**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate metrics by district\ncv_metrics <- all_predictions %>%\n  group_by(District) %>%\n  summarize(\n    MAE = mean(abs(countBurglaries - prediction)),\n    RMSE = sqrt(mean((countBurglaries - prediction)^2)),\n    ME = mean(countBurglaries - prediction)\n  )\n```\n:::\n\n\n**Calculating Kernel Density Estimation (KDE)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spatstat)\n\n# Step 1: Convert to point pattern (ppp) object\nburglary_ppp <- as.ppp(\n  X = st_coordinates(burglaries),\n  W = as.owin(st_bbox(chicago_boundary))\n)\n\n# Step 2: Calculate KDE\nkde_surface <- density.ppp(\n  burglary_ppp,\n  sigma = 1000,  # Bandwidth in meters\n  edge = TRUE    # Edge correction\n)\n\n# Step 3: Extract values to fishnet cells\nfishnet$kde_risk <- raster::extract(\n  raster(kde_surface),\n  st_centroid(fishnet)\n)\n\n# Standardize to 0-1 scale for comparison\nfishnet$kde_risk <- (fishnet$kde_risk - min(fishnet$kde_risk, na.rm=T)) / \n                     (max(fishnet$kde_risk, na.rm=T) - min(fishnet$kde_risk, na.rm=T))\n\n# Visualizing Model vs. KDE Performance\n# Bar chart comparing methods\ncomparison_data <- bind_rows(\n  model_results %>% mutate(Method = \"Negative Binomial Model\"),\n  kde_results %>% mutate(Method = \"Kernel Density\")\n)\n\nggplot(comparison_data, aes(x = risk_category, y = pct_of_total, fill = Method)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  scale_fill_manual(values = c(\"#440154FF\", \"#FDE724FF\")) +\n  labs(\n    title = \"Percentage of 2018 Burglaries Captured\",\n    subtitle = \"Which method performs better in high-risk areas?\",\n    x = \"Risk Category\",\n    y = \"% of Total 2018 Burglaries\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n:::\n\n\n## Questions & Challenges\n\n-   \n\n## Connections to Policy\n\n-   \n\n## Reflection\n\n-   Further Reading: <https://www.wired.com/story/how-peter-thiels-secretive-data-company-pushed-into-policing/>\n-   <https://blog.palantir.com/about-palantir-ddddb78aec29>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}